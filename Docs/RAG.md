
https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/
https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/
https://medium.com/the-ai-forum/rag-on-complex-pdf-using-llamaparse-langchain-and-groq-5b132bd1f9f3


existing
	https://github.com/mlc-delgado/pytldr-oss/
	https://github.com/abgulati/LARS/tree/v1.1
	https://github.com/infiniflow/ragflow
	https://github.com/kruxai/ragbuilder


RAG 101
	https://www.youtube.com/watch?v=nc0BupOkrhI
	https://arxiv.org/abs/2401.08406
	https://github.com/NirDiamant/RAG_Techniques?tab=readme-ov-file
	https://github.com/jxnl/n-levels-of-rag
	https://winder.ai/llm-architecture-rag-implementation-design-patterns/

201
	https://medium.com/@cdg2718/why-your-rag-doesnt-work-9755726dd1e9
	https://www.cazton.com/blogs/technical/advanced-rag-techniques
	https://medium.com/@krtarunsingh/advanced-rag-techniques-unlocking-the-next-level-040c205b95bc
	https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6
	https://winder.ai/llm-architecture-rag-implementation-design-patterns/
	https://towardsdatascience.com/17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8
	https://medium.com/@samarrana407/mastering-rag-advanced-methods-to-enhance-retrieval-augmented-generation-4b611f6ca99a
	https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0
	https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3
	https://pub.towardsai.net/rag-architecture-advanced-rag-3fea83e0d189?gi=47c0b76dbee0


To Read
	https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-ii-77b62bf8a5d3
	https://huggingface.co/papers/2406.12824

Walking Rag
	https://olickel.com/retrieval-augmented-research-1-basics
	https://olickel.com/retrieval-augmented-research-2-walking
	https://olickel.com/retrieval-augmented-research-3-use-the-whole-brain


Building
	https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1
	https://superlinked.com/vectorhub/articles/advanced-retrieval-augmented-generation
	https://opea.dev/
	https://archive.is/Nup8v
	https://techcommunity.microsoft.com/t5/microsoft-developer-community/building-the-ultimate-nerdland-podcast-chatbot-with-rag-and-llm/ba-p/4175577
	https://medium.com/@LakshmiNarayana_U/advanced-rag-techniques-in-ai-retrieval-a-deep-dive-into-the-chroma-course-d8b06118cde3
	https://rito.hashnode.dev/building-a-multi-hop-qa-with-dspy-and-qdrant
	https://blog.gopenai.com/advanced-retrieval-augmented-generation-rag-techniques-5abad385ac66?gi=09e684acab4d
	https://www.youtube.com/watch?v=bNqSRNMgwhQ
	https://www.youtube.com/watch?v=7h6uDsfD7bg
	https://www.youtube.com/watch?v=Balro-DxFyk&list=PLwPYSl1MQp4FpIzn48ypesKYzLvUBQpPF&index=5
	https://github.com/jxnl/n-levels-of-rag
	https://rito.hashnode.dev/building-a-multi-hop-qa-with-dspy-and-qdrant


Articles
	https://posts.specterops.io/summoning-ragnarok-with-your-nemesis-7c4f0577c93b?gi=7318858af6c3
	https://blog.demir.io/advanced-rag-implementing-advanced-techniques-to-enhance-retrieval-augmented-generation-systems-0e07301e46f4
	https://arxiv.org/abs/2312.10997
	https://jxnl.co/writing/2024/05/22/systematically-improving-your-rag/
	https://www.arcus.co/blog/rag-at-planet-scale
	https://d-star.ai/embeddings-are-not-all-you-need


Tables Extraction
	https://ai.gopubby.com/advanced-rag-retrieval-strategy-embedded-tables-fdb3e44003a5?gi=3a05c418031c

Chunking

Citations
	https://osu-nlp-group.github.io/AttributionBench/
	https://github.com/MadryLab/context-cite
	https://arxiv.org/abs/2305.14627

Context Compression
	https://arxiv.org/abs/2406.11357
	https://arxiv.org/abs/2406.06110
	https://arxiv.org/abs/2406.05317
	https://arxiv.org/abs/2405.17052
	https://arxiv.org/abs/2406.02376
	https://arxiv.org/abs/2405.18203
	https://arxiv.org/abs/2406.02376
	https://arxiv.org/abs/2407.08892
	https://arxiv.org/abs/2407.14057
	Long Context
		https://arxiv.org/abs/2404.10308
		https://arxiv.org/abs/2407.01370v1
		https://arxiv.org/html/2407.14482v1
		https://arxiv.org/abs/2406.07138

Vector Embeddings
	101
		https://www.youtube.com/watch?v=viZrOnJclY0
		https://aclanthology.org/W13-2322.pdf
		https://hamel.dev/blog/posts/evals/
		https://nextword.substack.com/p/vector-database-is-not-a-separate
		https://turso.tech/blog/sqlite-retrieval-augmented-generation-and-vector-search
	DB Usage
		https://medium.com/intel-tech/optimize-vector-databases-enhance-rag-driven-generative-ai-90c10416cb9c
		https://www.timescale.com/blog/pgvector-vs-pinecone/
		https://www.timescale.com/blog/how-we-made-postgresql-as-fast-as-pinecone-for-vector-data/
		https://stephencollins.tech/posts/how-to-use-sqLite-to-store-and-query-vector-embeddings
		https://turso.tech/blog/turso-brings-native-vector-search-to-sqlite	
	Finetune
		https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding/
		https://modal.com/blog/fine-tuning-embeddings
		https://huggingface.co/blog/train-sentence-transformers
		https://www.philschmid.de/fine-tune-embedding-model-for-rag
	Enhancements(?)
		https://arxiv.org/abs/2406.15241
		https://arxiv.org/abs/2407.09252
		https://towardsdatascience.com/how-to-reduce-embedding-size-and-increase-rag-retrieval-speed-7f903d3cecf7?gi=d994510fff0c
		https://arxiv.org/abs/2304.01982
	Models
		https://huggingface.co/google/xtr-base-multilingual
	SQLite
		https://github.com/asg017/sqlite-vec
		https://github.com/asg017/sqlite-lembed

Evals
	101
		https://docs.smith.langchain.com/concepts/evaluation#evaluating-a-single-step-of-an-agent
		https://thenewstack.io/openai-rag-vs-your-customized-rag-which-one-is-better/
	Benchmarks
		https://github.com/TIGER-AI-Lab/LongICLBench
		https://arxiv.org/html/2403.19889v1
		https://arxiv.org/pdf/2407.03651
		https://github.com/snorkel-ai/long-context-eval
		https://arxiv.org/html/2405.13622v1 - Tuning your search is better than a bigger model.
		https://arxiv.org/abs/2309.01431
		https://huggingface.co/papers/2406.10149
		https://github.com/booydar/babilong
		https://medium.com/@techsachin/ruler-benchmark-to-evaluate-long-context-modeling-capabilities-of-language-models-7eb13a269e36
	Evals
		https://github.com/Arize-ai/phoenix
		https://github.com/truera/trulens
		https://github.com/Marker-Inc-Korea/AutoRAG
		https://huggingface.co/learn/cookbook/en/rag_evaluation
		https://www.youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S
	Ragas
		https://www.youtube.com/watch?v=fWC4VxolWAk
		https://www.analyticsvidhya.com/blog/2024/05/a-beginners-guide-to-evaluating-rag-pipelines-using-ragas/
		https://github.com/explodinggradients/ragas
		https://docs.ragas.io/en/latest/index.html
		https://docs.ragas.io/en/latest/concepts/testset_generation.html
	LLM as a Judge
		https://cameronrwolfe.substack.com/p/llm-as-a-judge
		https://arxiv.org/abs/2407.10817
	SummRAG
		https://github.com/ncsulsj/Robust_Summarization
	Test Data
		https://docs.ragas.io/en/latest/concepts/testset_generation.html
		https://www.turingpost.com/p/sytheticdata


Graphrag
	101
		https://medium.com/microsoftazure/introducing-graphrag-with-langchain-and-neo4j-90446df17c1e
		https://www.youtube.com/watch?v=vX3A96_F3FU
		https://www.turingpost.com/p/graphrag
		https://div.beehiiv.com/p/knowledge-graphs-graphrag-advanced-intelligent-data-retrieval
	Build
		https://github.com/pingcap/tidb-vector-python/blob/main/examples/graphrag-step-by-step-tutorial/README.md
		https://github.com/anjor/naive-graphrag-impl
		https://ragaboutit.com/graph-rag-vs-vector-rag-a-comprehensive-tutorial-with-code-examples/
		https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a
		https://medium.com/@samarrana407/step-by-step-guide-to-build-rag-application-with-knowledge-graph-456fbaf64e98
		https://github.com/KaifAhmad1/RAG-with-KnowledgeGraph/blob/main/RAG_with_Graph_Database.ipynb


Hallucinations
	Benchmarks
		https://arxiv.org/abs/2407.17468
		https://huggingface.co/datasets/PatronusAI/HaluBench
		https://github.com/rungalileo/hallucination-index
	Luna
		https://arxiv.org/abs/2406.00975v2
		Paid for only: https://www.rungalileo.io/blog/introducing-galileo-luna-a-family-of-evaluation-foundation-models
	Patronus
		https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct
		https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct


Knowledge Graphs
	101
		https://towardsdatascience.com/combine-text-embeddings-and-knowledge-graph-embeddings-in-rag-systems-5e6d7e493925?gi=fdc5811c4a92
		https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/
		https://archive.is/E1bdb
	Building one
		https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/
		https://towardsdatascience.com/text-to-knowledge-graph-made-easy-with-graph-maker-f3f890c0dbe8?gi=f557597a37c9
		https://blog.dagworks.io/p/building-a-conversational-graphdb
		https://generativeai.pub/knowledge-graph-extraction-visualization-with-local-llm-from-unstructured-text-a-history-example-94c63b366fed?gi=829d63e4e825
		https://www.pingcap.com/article/graphrag-enhancing-traditional-rag-through-knowledge-graph
		https://neo4j.com/developer-blog/knowledge-graph-rag-application/
		https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/
		https://github.com/rahulnyk/knowledge_graph
		https://github.com/neo4j-labs/llm-graph-builder
		https://medium.com/@samarrana407/step-by-step-guide-to-build-rag-application-with-knowledge-graph-456fbaf64e98
		https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/
	Built
		https://github.com/BaranziniLab/KG_RAG
	Neo4j
		https://github.com/neo4j/NaLLM
		https://github.com/chrisammon3000/dspy-neo4j-knowledge-graph/blob/main/src/neo4j.py
	Customer chatbots
		Augmenting RAG Systems with Knowledge Graphs for Customer Service Q&A - https://archive.is/3kmQl
		https://www.youtube.com/watch?v=a6VWF_DpWDU
		https://www.youtube.com/watch?v=QSZHGGRouIE
	Papers
		https://arxiv.org/pdf/2407.00653
	Combining text embeddings + Graph embeddings
		https://archive.is/4wPGs
	SeaKR
		https://arxiv.org/html/2406.19215v1
		https://github.com/THU-KEG/SeaKR


LongRAG
	https://github.com/TIGER-AI-Lab/LongRAG


Named Entity Recognition
	https://github.com/urchade/GLiNER


re-rankers
	101
		https://www.elastic.co/search-labs/blog/semantic-reranking-with-retrievers
		https://www.pinecone.io/learn/series/rag/rerankers/
	RRF
		https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf
		https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking
	Papers
		https://arxiv.org/abs/2407.02485v1
	Models
		https://huggingface.co/BAAI/bge-reranker-base
	RankRAG
		https://arxiv.org/abs/2407.02485
	G-RAG
		https://arxiv.org/abs/2405.18414




Search
	101
		https://softwaredoug.com/blog/2024/06/25/what-ai-engineers-need-to-know-search
		https://arxiv.org/abs/2104.05740
		https://www.linkedin.com/pulse/googles-new-algorithms-just-made-searching-vector-faster-bamania-cyx3e/
		https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167
	Sample Builds
		https://simonwillison.net/2024/Jun/21/search-based-rag/
	Beam Search
		https://www.width.ai/post/what-is-beam-search
	BM25
		https://docs.haystack.deepset.ai/docs/inmemorybm25retriever
		https://jdsemrau.substack.com/p/semantic-search-over-200k-posts
		https://huggingface.co/papers/2407.03618
		https://github.com/xhluca/bm25s
	DBs
		https://github.com/infiniflow/infinity
	RAG + BM25 =  better than either alone - https://about.xethub.com/blog/you-dont-need-a-vector-database
	Tree Search
		https://jykoh.com/search-agents/paper.pdf
		https://jykoh.com/search-agents
		https://arxiv.org/abs/2407.00320
	Blended RAG / Hybrid Search
		https://arxiv.org/html/2404.07220v1
		https://www.youtube.com/watch?v=kOALKZvhMgQ
		https://github.com/Rman410/hybrid-search/blob/main/hybrid-search.py
		https://github.com/pgvector/pgvector-python/blob/master/examples/hybrid_search.py
		https://infiniflow.org/blog/best-hybrid-search-solution
	SPLADE
		https://www.pinecone.io/learn/splade/
	SOAR
		https://research.google/blog/soar-new-algorithms-for-even-faster-vector-search-with-scann/

Speculative RAG
	https://arxiv.org/abs/2407.08223

Spreadsheets
	https://huggingface.co/papers/2407.09025



Elastic
https://www.elastic.co/search-labs/blog/elasticsearch-vector-large-scale-part1
https://www.elastic.co/search-labs/blog/rag-playground-introduction
https://www.elastic.co/search-labs/blog/elasticsearch-opensearch-vector-search-performance-comparison
https://www.elastic.co/search-labs/blog/advanced-chunking-fetch-surrounding-chunks

Google
https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-1
https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-search-adds-new-generative-ai-capabilities
https://cloud.google.com/blog/products/ai-machine-learning/rag-and-grounding-on-vertex-ai
https://cloud.google.com/generative-ai-app-builder/docs/builder-apis


Langchain
https://www.youtube.com/watch?v=-ROS6gfYIts


Zilliz
https://zilliz.com/learn/LangChain
https://zilliz.com/learn/Retrieval-Augmented-Generation
https://zilliz.com/learn/introduction-to-unstructured-data
https://zilliz.com/learn/vector-database-backup-and-recovery-safeguard-data-integrity
https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings
https://zilliz.com/learn/raft-or-not
https://zilliz.com/learn/open-source-vector-database-benchmarking-your-way
https://zilliz.com/learn/image-based-trademark-similarity-search-system

Existing solutions
https://github.com/brianpetro/obsidian-smart-connections
https://github.com/s-kostyaev/elisa
https://github.com/EdwardDali/erag
https://github.com/whitead/paper-qa
https://github.com/pymupdf/RAG
https://github.com/MichaelMcCulloch/WikiDex
https://r2r-docs.sciphi.ai/introduction
https://github.com/s-kostyaev/elisa
https://github.com/severian42/GraphRAG-Local-UI
https://github.com/AI-Commandos/RAGMeUp
https://github.com/EdwardDali/erag
https://github.com/foldl/chatllm.cpp/blob/master/docs/rag.md
https://betterprogramming.pub/building-a-reddit-thread-summarizer-with-chatgpt-api-5b0dcd50b88e?gi=157029dc5472
https://github.com/wordsmith-ai/hello-wordsmith
https://github.com/Mozilla-Ocho/llamafile-rag-example

https://github.com/Azure-Samples/rag-postgres-openai-python
https://github.com/Azure-Samples/rag-postgres-openai-python/blob/e30ea96ca11ca6578ca38d3428594bd98d704900/src/fastapi_app/postgres_searcher.py#L38
https://github.com/SciPhi-AI/R2R
https://huggingface.co/spaces/Prat0/ClarifyAI
https://github.com/SylphAI-Inc/LightRAG
	https://lightrag.sylph.ai/developer_notes/lightrag_design_philosophy.html
https://neuml.hashnode.dev/build-rag-pipelines-with-txtai
https://fujia19990407.github.io/project_website/


# RAG Notes


Corrective RAG (CRAG)
Self-reflective RAG
	https://blog.langchain.dev/agentic-rag-with-langgraph/

https://github.com/NirDiamant/RAG_Techniques?tab=readme-ov-file

Improvements
	https://www.reddit.com/r/LocalLLaMA/comments/1e84de2/top_enhancements_to_try_once_you_have_a_vanilla/


### Building my RAG Solution
- **Outline**
	* Modular architecture design
- **Pre-Retrieval**
	* F
- **Retrieval**
	* F
- **Post-Retrieval**
	* 
- **Generation & Post-Generation** 
	- Prompt Compression
		* https://github.com/microsoft/LLMLingua
	- **Citations**
		* Contextcite: https://github.com/MadryLab/context-cite



### RAG Process
1. Pre-Retrieval
	- Raw data creation / Preparation
		1. Prepare data so that text-chunks are self-explanatory
2. **Retrieval**
	1. **Chunk Optimization**
		- Naive - Fixed-size (in characters) Overlapping Sliding windows
			* `limitations include imprecise control over context size, the risk of cutting words or sentences, and a lack of semantic consideration. Suitable for exploratory analysis but not recommended for tasks requiring deep semantic understanding.`
		- Recursive Structure Aware Splitting
			* `A hybrid method combining fixed-size sliding window and structure-aware splitting. It attempts to balance fixed chunk sizes with linguistic boundaries, offering precise context control. Implementation complexity is higher, with a risk of variable chunk sizes. Effective for tasks requiring granularity and semantic integrity but not recommended for quick tasks or unclear structural divisions.`
		- Structure Aware Splitting (by sentence/paragraph)
			* ` Respecting linguistic boundaries preserves semantic integrity, but challenges arise with varying structural complexity. Effective for tasks requiring context and semantics, but unsuitable for texts lacking defined structural divisions.`
		- Context-Aware Splitting (Markdown/LaTeX/HTML)
			* `ensures content types are not mixed within chunks, maintaining integrity. Challenges include understanding specific syntax and unsuitability for unstructured documents. Useful for structured documents but not recommended for unstructured content.`
		- NLP Chunking: Tracking Topic Changes
			* `based on semantic understanding, dividing text into chunks by detecting significant shifts in topics. Ensures semantic consistency but demands advanced NLP techniques. Effective for tasks requiring semantic context and topic continuity but not suitable for high topic overlap or simple chunking tasks.`
	2. **Enhancing Data Quality**
		- Abbreviations/technical terms/links
			* `To mitigate that issue, we can try to ingest that necessary additional context while processing the data, e.g. replace abbreviations with the full text by using an abbreviation translation table.`
	3. **Meta-data**
		- You can add metadata to your vector data in all vector databases. Metadata can later help to (pre-)filter the entire vector database before we perform a vector search.
	4. **Optimize Indexing Structure**
		* `Full Search vs. Approximate Nearest Neighbor, HNSW vs. IVFPQ`
		1. Chunk Optimization
			* Semantic splitter - optimize chunk size used for embedding
		2. **Multi-Representation Indexing** - Convert into compact retrieval units (i.e. summaries)
			1. Parent Document
			2. Dense X
		3. **Specialized Embeddings**
			1. Fine-tuned
			2. ColBERT
		4. **Heirarchical Indexing** - Tree of document summarization at various abstraction levels
			1. **RAPTOR** - Recursive Abstractive Processing for Tree-Organized Retrieval
				* https://arxiv.org/pdf/2401.18059
				* `RAPTOR is a novel tree-based retrieval system designed for recursively embedding, clustering, and summarizing text segments. It constructs a tree from the bottom up, offering varying levels of summarization. During inference, RAPTOR retrieves information from this tree, incorporating data from longer documents at various levels of abstraction.`
				* https://archive.is/Zgb13 - README
		5. **GraphRAG** - Use an LLM to construct a graph-based text index
			* https://arxiv.org/pdf/2404.16130
			* https://github.com/microsoft/graphrag
			- Occurs in two steps:
				1. Derives a knowledge graph from the source documents
				2. Generates community summaries for all closely connected entity groups
			* Given a query, each community summary contributes to a partial response. These partial responses are then aggregated to form the final global answer.
			- Workflow:
				1. Chunk Source documents
				2. Construct a knowledge graph by extracting entities and their relationships from each chunk.
				3. Simultaneously, Graph RAG employs a multi-stage iterative process. This process requires the LLM to determine if all entities have been extracted, similar to a binary classification problem.
				4. Element Instances → Element Summaries → Graph Communities → Community Summaries
					* Graph RAG employs community detection algorithms to identify community structures within the graph, incorporating closely linked entities into the same community. 
					* `In this scenario, even if LLM fails to identify all variants of an entity consistently during extraction, community detection can help establish the connections between these variants. Once grouped into a community, it signifies that these variants refer to the same entity connotation, just with different expressions or synonyms. This is akin to entity disambiguation in the field of knowledge graph.`
					* `After identifying the community, we can generate report-like summaries for each community within the Leiden hierarchy. These summaries are independently useful in understanding the global structure and semantics of the dataset. They can also be used to comprehend the corpus without any problems.`
				5. Community Summaries → Community Answers → Global Answer
		6. **HippoRAG**
			* https://github.com/OSU-NLP-Group/HippoRAG
			* https://arxiv.org/pdf/2405.14831
			* https://archive.is/Zgb13#selection-2093.24-2093.34
		7. **spRAG/dsRAG** - README
			* https://github.com/D-Star-AI/dsRAG
	5. **Choosing the right embedding model**
		* F.
	6. **Self query**
		* https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/
	7. **Hybrid & Filtered Vector Search**
		* Perform multiple search methods and combine results together
		1. Keyword Search(BM25) + Vector
		2. f
	8. **Query Construction**
		- Create a query to interact with a specific DB
		1. Text-to-SQL
			* Relational DBs
			* Rewrite a query into a SQL query
		2. Text-to-Cyber
			* Graph DBs
			* Rewrite a query into a cypher query
		3. Self-query Retriever
			* Vector DBs
			* Auto-generate metadata filters from query 
	9. **Query Translation**
		1. Query Decomposition - Decompose or re-phrase the input question
			1. Multi-Query
				* https://archive.is/5y4iI
				- Sub-Question Querying
					* `The core idea of the sub-question strategy is to generate and propose sub-questions related to the main question during the question-answering process to better understand and answer the main question. These sub-questions are usually more specific and can help the system to understand the main question more deeply, thereby improving retrieval accuracy and providing correct answers.`
					1. First, the sub-question strategy generates multiple sub-questions from the user query using LLM (Large Language Model).
					2. Then, each sub-question undergoes the RAG process to obtain its own answer (retrieval generation).
					3. Finally, the answers to all sub-questions are merged to obtain the final answer. 
			2. Step-Back Prompting
				* http://arxiv.org/pdf/2310.06117
				* `technique that guides LLM to extract advanced concepts and basic principles from specific instances through abstraction, using these concepts and principles to guide reasoning. This approach significantly improves LLM’s ability to follow the correct reasoning path to solve problems.`
				- Flow:
					1. Take in a question - `Estella Leopold went to what school in Aug 1954 and Nov 1954?`
					2. Create a(or multiple) stepback question - `What was Estella Leopold's education history?`
					3. Answer Stepback answer
					4. Perform reasoning using stepback question + answer to create final answer
			3. RAG-Fusion - Combining multiple data sources in one RAG (Walking RAG?) 
				- 3 parts: 
					1. Query Generation - Generate multiple sub-queries from the user’s input to capture diverse perspectives and fully understand the user’s intent.
					2. Sub-query Retrieval - Retrieve relevant information for each sub-query from large datasets and repositories, ensuring comprehensive and in-depth search results.
					3. Reciprocal Rank Fusion - Merge the retrieved documents using Reciprocal Rank Fusion (RRF) to combine their ranks, prioritizing the most relevant and comprehensive results.
		2. Pseudo-Documents - Hypothetical documents
			1. HyDE
				* https://arxiv.org/abs/2212.10496
	10. **Query Enhancement / Rewriting**
		- Replacing Acronyms with full phrasing
		- Providing synonyms to industry terms
		- Literally just ask the LLM to do it.
	11. **Query Extension**
	12. **Query Expansion**
		1. Query Expansion with generated answers
			* `We use the LLM to generate an answer, before performing the similarity search. If it is a question that can only be answered using our internal knowledge, we indirectly ask the model to hallucinate, and use the hallucinated answer to search for content that is similar to the answer and not the user query itself.`
			- Implementations:
				- HyDE (Hypothetical Document Embeddings)
				- Rewrite-Retrieve-Read
				- Step-Back Prompting
				- Query2Doc
				- ITER-RETGEN
				- Others?
	13. **Multiple System Prompts**
		* Generate multiple prompts, consolidate answer
	14. **Query Routing** - Let LLM decide which datastore to use for information retrieval based on user's query
		1. Logical Routing - Let LLM choose DB based on question
		2. Semantic Routing - embed question and choose prompt based on similarity
	15. **Response Summarization** - Using summaries of returned items 
	16. **Ranking***
			1. Re-Rank
				* https://div.beehiiv.com/p/advanced-rag-series-retrieval
			2. RankGPT
			3. RAG-Fusion
		17. **Refinement**
			1. CRAG
				* https://arxiv.org/pdf/2401.15884
				* https://medium.com/@kbdhunga/corrective-rag-c-rag-and-control-flow-in-langgraph-d9edad7b5a2c
				* https://medium.com/@djangoist/how-to-create-accurate-llm-responses-on-large-code-repositories-presenting-cgrag-a-new-feature-of-e77c0ffe432d
	18. **Active Retrieval** - re-retrieve and or retrieve from new data sources if retrieved documents are not relevant.
		1. CRAG
3. **Post-Retrieval**
	1. **Context Enrichment**
		1. Sentence Window Retriever
			* `The text chunk with the highest similarity score represents the best-matching content found. Before sending the content to the LLM we add the k-sentences before and after the text chunk found. This makes sense since the information has a high probability to be connected to the middle part and maybe the piece of information in the middle text chunk is not complete.`
		2. Auto-Merging Retriever
			* `The text chunk with the highest similarity score represents the best-matching content found. Before sending the content to the LLM we add  each small text chunk's assigned  “parent” chunks, which do not necessarily have to be the chunk before and after the text chunk found.`
			* We can build on top of that concept and set up a whole hierarchy like a decision tree with different levels of Parent Nodes, Child Nodes and Leaf Nodes. We could for example have 3 levels, with different chunk sizes - See https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/
4. **Generation & Post-Generation**
	1. **Self-RAG**
		* https://github.com/AkariAsai/self-rag
	2. **Rewrite-Retrieve-Read (RRR)**
		* https://arxiv.org/pdf/2305.14283 
	3. **Choosing the appropriate/correct model**
	4. **Agents**
	5. **Evaluation**
		- Metrics:
			- Generation
				1. Faithfulness - How factually accurate is the generated answer?
				2. Answer Relevancy - How relevant is the generated answer to the question?
			- Retrieval
				1. Context Precision
				2. Context Recall
			- Others
				1. Answer semantic Similarity
				2. Answer correctness
		1. Normalized Discounted Cumulative Gain (NDCG)
			* https://www.evidentlyai.com/ranking-metrics/ndcg-metric#:~:text=DCG%20measures%20the%20total%20item,ranking%20quality%20in%20the%20dataset.
		2. Existing RAG Eval Frameworks
			* RAGAS - https://archive.is/I8f2w
		3. LLM as a Judge
			* We generate an evaluation dataset -> Then define a so-called critique agent with suitable criteria we want to evaluate -> Set up a test pipeline that automatically evaluates the responses of the LLMs based on the defined criteria.
		4. Usage Metrics
			* Nothing beats real-world data.
5. **Delivery**




RAG-Fusion - Combining multiple data source in one RAG search


JSON file store Vector indexing

### Chunking - https://github.com/D-Star-AI/dsRAG'
- **Improvements/Ideas**
	* As part of chunk header summary, include where in the document this chunk is located, besides chunk #x, so instead this comes from the portion of hte document talking about XYZ in the greater context
- Chunk Headers
	* The idea here is to add in higher-level context to the chunk by prepending a chunk header. This chunk header could be as simple as just the document title, or it could use a combination of document title, a concise document summary, and the full hierarchy of section and sub-section titles.
- Chunks -> segments*
	* Large chunks provide better context to the LLM than small chunks, but they also make it harder to precisely retrieve specific pieces of information. Some queries (like simple factoid questions) are best handled by small chunks, while other queries (like higher-level questions) require very large chunks.
	* We break documents up into chunks with metadata at the head of each chunk to help categorize it to the document/align it with the greater context
- **Semantic Sectioning**
	* Semantic sectioning uses an LLM to break a document into sections. It works by annotating the document with line numbers and then prompting an LLM to identify the starting and ending lines for each “semantically cohesive section.” These sections should be anywhere from a few paragraphs to a few pages long. The sections then get broken into smaller chunks if needed. The LLM is also prompted to generate descriptive titles for each section. These section titles get used in the contextual chunk headers created by AutoContext, which provides additional context to the ranking models (embeddings and reranker), enabling better retrieval.
	1. Identify sections
	2. Split sections into chunks
	3. Add metadata header to each chunk
		* `Document: X`
		* `Section: X1`
		* Alt: `Concise parent document summary`
		* Other approaches/bits of info can help/experiment...
- **AutoContext**
	* `AutoContext creates contextual chunk headers that contain document-level and section-level context, and prepends those chunk headers to the chunks prior to embedding them. This gives the embeddings a much more accurate and complete representation of the content and meaning of the text. In our testing, this feature leads to a dramatic improvement in retrieval quality. In addition to increasing the rate at which the correct information is retrieved, AutoContext also substantially reduces the rate at which irrelevant results show up in the search results. This reduces the rate at which the LLM misinterprets a piece of text in downstream chat and generation applications.`
- **Relevant Segment Extraction**
	* Relevant Segment Extraction (RSE) is a query-time post-processing step that takes clusters of relevant chunks and intelligently combines them into longer sections of text that we call segments. These segments provide better context to the LLM than any individual chunk can. For simple factual questions, the answer is usually contained in a single chunk; but for more complex questions, the answer usually spans a longer section of text. The goal of RSE is to intelligently identify the section(s) of text that provide the most relevant information, without being constrained to fixed length chunks.
- **Topic Aware Chunking by Sentence**
	* https://blog.gopenai.com/mastering-rag-chunking-techniques-for-enhanced-document-processing-8d5fd88f6b72?gi=2f39fdede29b


### Vector DBs
- Indexing mechanisms
	*  Locality-Sensitive Hashing (LSH)
	* Hierarchical Graph Structure
	* Inverted File Indexing
	* Product Quantization
	* Spatial Hashing
	* Tree-Based Indexing variations
- Embedding algos
	* Word2Vec
	* GloVe
	* Ada
	* BERT
	* Instructor
- Similarity Measurement Algos
	* Cosine similarity - measuring the cosine of two angles
	* Euclidean distance - measuring the distance between two points
- Indexing and Searching Algos
	- Approximate Nearest Neighbor (ANN)
		* FAISS
		* Annoy
		* IVF
		* HNSW
- Vector Similarity Search
	- `Inverted File (IVF)` - `indexes are used in vector similarity search to map the query vector to a smaller subset of the vector space, reducing the number of vectors compared to the query vector and speeding up Approximate Nearest Neighbor (ANN) search. IVF vectors are efficient and scalable, making them suitable for large-scale datasets. However, the results provided by IVF vectors are approximate, not exact, and creating an IVF index can be resource-intensive, especially for large datasets.`
	- `Hierarchical Navigable Small World (HNSW)` - `graphs are among the top-performing indexes for vector similarity search. HNSW is a robust algorithm that produces state-of-the-art performance with fast search speeds and excellent recall. It creates a multi-layered graph, where each layer represents a subset of the data, to quickly traverse these layers to find approximate nearest neighbors. HNSW vectors are versatile and suitable for a wide range of applications, including those that require high-dimensional data spaces. However, the parameters of the HNSW algorithm can be tricky to tune for optimal performance, and creating an HNSW index can also be resource intensive.`
- **Vectorization Process**
	- Usually several stages:
		1. Data Pre-processing
			* `The initial stage involves preparing the raw data. For text, this might include tokenization (breaking down text into words or phrases), removing stop words, and normalizing the text (like lowercasing). For images, preprocessing might involve resizing, normalization, or augmentation.`
		2. Feature Extraction
			* `The system extracts features from the preprocessed data. In text, features could be the frequency of words or the context in which they appear. For images, features could be various visual elements like edges, textures, or color histograms.`
		3. Embedding Generation
			* `Using algorithms like Word2Vec for text or CNNs for images, the extracted features are transformed into numerical vectors. These vectors capture the essential qualities of the data in a dense format, typically in a high-dimensional space.`
		4. Dimensionality Reduction
			* `Sometimes, the generated vectors might be very high-dimensional, which can be computationally intensive to process. Techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) are used to reduce the dimensionality while preserving as much of the significant information as possible.`
		5. Normalization
			* `Finally, the vectors are often normalized to have a uniform length. This step ensures consistency across the dataset and is crucial for accurately measuring distances or similarities between vectors.`



### Semantic Re-Ranker
* `enhances retrieval quality by re-ranking search results based on deep learning models, ensuring the most relevant results are prioritized.`
- General Steps
	1. Initial Retrieval: a query is processed, and a set of potentially relevant results is fetched. This set is usually larger and broader, encompassing a wide array of documents or data points that might be relevant to the query.
    2. LLM / ML model used to identify relevance
    3. Re-Ranking Process: In this stage, the retrieved results are fed into the deep learning model along with the query. The model assesses each result for its relevance, considering factors such as semantic similarity, context matching, and the query's intent.
    4. Generating a Score: Each result is assigned a relevance score by the model. This scoring is based on how well the content of the result matches the query in terms of meaning, context, and intent.
    5. Sorting Results: Based on the scores assigned, the results are then sorted in descending order of relevance. The top-scoring results are deemed most relevant to the query and are presented to the user.
    6. Continuous Learning and Adaptation: Many Semantic Rankers are designed to learn and adapt over time. By analyzing user interactions with the search results (like which links are clicked), the Ranker can refine its scoring and sorting algorithms, enhancing its accuracy and relevance.
- **Relevance Metrics**
- List of:
    1. Precision and Recall: These are fundamental metrics in information retrieval. Precision measures the proportion of retrieved documents that are relevant, while recall measures the proportion of relevant documents that were retrieved. High precision means that most of the retrieved items are relevant, and high recall means that most of the relevant items are retrieved.
    2. F1 Score: The F1 Score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, useful in scenarios where it's important to find an equilibrium between finding as many relevant items as possible (recall) and ensuring that the retrieved items are mostly relevant (precision).
    3. Normalized Discounted Cumulative Gain (NDCG): Particularly useful in scenarios where the order of results is important (like web search), NDCG takes into account the position of relevant documents in the result list. The more relevant documents appearing higher in the search results, the better the NDCG.
    4. Mean Average Precision (MAP): MAP considers the order of retrieval and the precision at each rank in the result list. It’s especially useful in tasks where the order of retrieval is important but the user is likely to view only the top few results.



### Issues in RAG
1. Indexing
	- Issues:
		1. Chunking
			1. Relevance & Precision
				* `Properly chunked documents ensure that the retrieved information is highly relevant to the query. If the chunks are too large, they may contain a lot of irrelevant information, diluting the useful content. Conversely, if they are too small, they might miss the broader context, leading to accurate responses but not sufficiently comprehensive.`
			2. Efficiency & Performance
				* `The size and structure of the chunks affect the efficiency of the retrieval process. Smaller chunks can be retrieved and processed more quickly, reducing the overall latency of the system. However, there is a balance to be struck, as too many small chunks can overwhelm the retrieval system and negatively impact performance.`
			3. Quality of Generation
				* `The quality of the generated output heavily depends on the input retrieved. Well-chunked documents ensure that the generator has access to coherent and contextually rich information, which leads to more informative, coherent, and contextually appropriate responses.`
			4. Scalability
				* `As the corpus size grows, chunking becomes even more critical. A well-thought-out chunking strategy ensures that the system can scale effectively, managing more documents without a significant drop in retrieval speed or quality.`
		1. Incomplete Content Representation
			* `The semantic information of chunks is influenced by the segmentation method, resulting in the loss or submergence of important information within longer contexts.`
		2. Inaccurate Chunk Similarity Search.
			* `As data volume increases, noise in retrieval grows, leading to frequent matching with erroneous data, making the retrieval system fragile and unreliable.`
		3. Unclear Reference Trajectory.
			* `The retrieved chunks may originate from any document, devoid of citation trails, potentially resulting in the presence of chunks from multiple different documents that, despite being semantically similar, contain content on entirely different topics.`
	- Potential Solutions
		- Chunk Optimization
			- Sliding window
				* overlapping chunks
			- Small to Big
 				* Retrieve small chunks then collect parent from meta data
 			- Enhance data granularity - apply data cleaning techniques, like removing irrelevant information, confirming factual accuracy, updating outdated information, etc.
 			- Adding metadata, such as dates, purposes, or chapters, for filtering purposes.
		- Structural Organization
			- Heirarchical Index
				* `In the hierarchical structure of documents, nodes are arranged in parent-child relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.`
				- Methods for constructing index:
					1. Structural awareness - paragraph and sentence segmentation in docs
					2. Content Awareness - inherent structure in PDF, HTML, Latex
					3. Semantic Awareness - Semantic recognition and segmentation of text based on NLP techniques, such as leveraging NLTK.
					4. Knowledge Graphs
2. Pre-Retrieval
	- Issues:
		- Poorly worded queries
		- Language complexity and ambiguity
	- Potential Solutions:
		- Multi-Query - Expand original question into multiple
		- Sub-Query - `The process of sub-question planning represents the generation of the necessary sub-questions to contextualize and fully answer the original question when combined. `
		- Chain-of-Verification(CoVe) - The expanded queries undergo validation by LLM to achieve the effect of reducing hallucinations. Validated expanded queries typically exhibit higher reliability.
			* https://arxiv.org/abs/2309.11495 
		- Query Transformation
			- Rewrite
				* The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. Therefore, we can prompt LLM to rewrite the queries.
			- HyDE
				* `When responding to queries, LLM constructs hypothetical documents (assumed answers) instead of directly searching the query and its computed vectors in the vector database. It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. In addition, it also includes Reverse HyDE, which focuses on retrieval from query to query.`
				* https://medium.aiplanet.com/advanced-rag-improving-retrieval-using-hypothetical-document-embeddings-hyde-1421a8ec075a?gi=b7fa45dc0f32&source=post_page-----e69b32dc13a3--------------------------------
			- Step-back prompting
				* https://arxiv.org/abs/2310.06117
				* https://cobusgreyling.medium.com/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb
		- Query Routing
			* Based on varying queries, routing to distinct RAG pipeline,which is suitable for a versatile RAG system designed to accommodate diverse scenarios.
			- Metadata Router/Filter
				* `involves extracting keywords (entity) from the query, followed by filtering based on the keywords and metadata within the chunks to narrow down the search scope.`
			- Semantic Router
				* https://medium.com/ai-insights-cobet/beyond-basic-chatbots-how-semantic-router-is-changing-the-game-783dd959a32d
		- Query Construction
			- Text-to-Cypher
			- Text-to-SQL
			* https://blog.langchain.dev/query-construction/?source=post_page-----e69b32dc13a3--------------------------------
3. Retrieval
	- 3 Main considerations:
		1. Retrieval Efficiency
		2. Embedding Quality
		3. Alignment of tasks, data and models
	- Sparse Retreiver
		* EX: BM25, TF-IDF
	- Dense Retriever
		* ColBERT
		* BGE/Cohere embedding/OpenAI-Ada-002
	- Retriever Fine-tuning
		- SFT
		- LSR (LM-Supervised Retriever)
		- Reinforcement learning
		- Adapter
			* https://arxiv.org/pdf/2310.18347
			* https://arxiv.org/abs/2305.17331
		`
4. Post-Retrieval
	- Primary Challenges:
		1. Lost in the middle
		2. Noise/anti-fact chunks
		3. Context windows.
	- Potential Solutions
		- Re-Rank
			* Re-rank implementation: https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5
			- Rule-based re-rank
				* According to certain rules, metrics are calculated to rerank chunks.
				* Some: Diversity; Relevance; MRR (Maximal Marginal Relevance, 1998)
			- Model based rerank
				* Utilize a language model to reorder the document chunks
		- Compression & Selection
			- LLMLingua
				* https://github.com/microsoft/LLMLingua
				* https://llmlingua.com/
			- RECOMP
				* https://arxiv.org/pdf/2310.04408
			- Selective Context
				* https://aclanthology.org/2023.emnlp-main.391.pdf
			- Tagging Filter
				* https://python.langchain.com/v0.1/docs/use_cases/tagging/
			- LLM Critique
5. Generator
	* Utilize the LLM to generate answers based on the user’s query and the retrieved context information.
	- Finetuning
		* SFT
		* RL
		* Distillation
		- Dual FT
			* `In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. Fine-tuning the retriever and the generator separately separately belongs to the combination of the former two, rather than being part of Dual FT.`
			* https://arxiv.org/pdf/2310.01352
6. Orchestration
	* `Orchestration refers to the modules used to control the RAG process. RAG no longer follows a fixed process, and it involves making decisions at key points and dynamically selecting the next step based on the results.`
	- Scheduling
		* `The Judge module assesses critical point in the RAG process, determining the need to retrieve external document repositories, the satisfaction of the answer, and the necessity of further exploration. It is typically used in recursive, iterative, and adaptive retrieval.`
		- `Rule-base`
			* `The next course of action is determined based on predefined rules. Typically, the generated answers are scored, and then the decision to continue or stop is made based on whether the scores meet predefined thresholds. Common thresholds include confidence levels for tokens.`
		- `Prompt-base`
			* `LLM autonomously determines the next course of action. There are primarily two approaches to achieve this. The first involves prompting LLM to reflect or make judgments based on the conversation history, as seen in the ReACT framework. The benefit here is the elimination of the need for fine-tuning the model. However, the output format of the judgment depends on the LLM’s adherence to instructions.`
			* https://arxiv.org/pdf/2305.06983
		- Tuning based
			 * The second approach entails LLM generating specific tokens to trigger particular actions, a method that can be traced back to Toolformer and is applied in RAG, such as in Self-RAG.
			 * https://arxiv.org/pdf/2310.11511
	- Fusion
		* `This concept originates from RAG Fusion. As mentioned in the previous section on Query Expansion, the current RAG process is no longer a singular pipeline. It often requires the expansion of retrieval scope or diversity through multiple branches. Therefore, following the expansion to multiple branches, the Fusion module is relied upon to merge multiple answers.`
		- Possibility Ensemble
			* `The fusion method is based on the weighted values of different tokens generated from multiple beranches, leading to the comprehensive selection of the final output. Weighted averaging is predominantly employed.`
			* https://arxiv.org/pdf/2301.12652
		- Reciprocal Rank Fusion
			* `RRF, is a technique that combines the rankings of multiple search result lists to generate a single unified ranking. Developed in collaboration with the University of Waterloo (CAN) and Google, RRF produces results that are more effective than reordering chunks under any single branch.`
			* https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1
- Semantic dissonance
	* `the discordance between your task’s intended meaning, the RAG’s understanding of it, and the underlying knowledge that’s stored.`
- Poor explainability of embeddings
- Semantic Search tends to be directionally correct but inherently fuzzy
	* Good for finding top-k results
- Significance of Dimensionality in Vector Embeddings
	* `The dimensionality of a vector, which is the length of the vector, plays a crucial role. Higher-dimensional vectors can capture more information and subtle nuances of the data, leading to more accurate models. However, higher dimensionality also increases computational complexity. Therefore, finding the right balance in vector dimensionality is key to efficient and effective model performance.`


### Potential Improvements when building
https://gist.github.com/Donavan/62e238aa0a40ca88191255a070e356a2
- **Chunking**
	- Relevance & Precision
	- Efficiency and Performance
	- Quality of Generation
	- Scalability
- **Embeddings**
	1. **Encoder Fine-Tuning**
		* `Despite the high efficiency of modern Transformer Encoders, fine-tuning can still yield modest improvements in retrieval quality, especially when tailored to specific domains.`
	2. Ranker Fine-Tuning
		* `Employing a cross-encoder for re-ranking can refine the selection of context, ensuring that only the most relevant text chunks are considered.`
	3. LLM Fine-Tuning
		* `The advent of LLM fine-tuning APIs allows for the adaptation of models to specific datasets or tasks, enhancing their effectiveness and accuracy in generating responses.`
- **Constructing the Search Index**
	1. **Vector store index**
	2. **Heirarchical Indices**
		* Two-tiered index, one for doc summaries the other for detailed chunks
		* Filter through the summaries first then search the chunks
	3. **Hypothetical Questions and HyDE approach**
		* A novel approach involves the generation of hypothetical questions for each text chunk. These questions are then vectorized and stored, replacing the traditional text vectors in the index. This method enhances semantic alignment between user queries and stored data, potentially leading to more accurate retrievals. The HyDE method reverses this process by generating hypothetical responses to queries, using these as additional data points to refine search accuracy.
- **Context Enrichment**
	1. **Sentence-Window retrieval**
		* `This technique enhances search precision by embedding individual sentences and extending the search context to include neighboring sentences. This not only improves the relevance of the retrieved data but also provides the LLM with a richer context for generating responses.`
	2. **Auto-merging Retriever** (Parent Document Retriever)
		* `Similar to the Sentence Window Retrieval, this method focuses on granularity but extends the context more broadly. Documents are segmented into a hierarchy of chunks, and smaller, more relevant pieces are initially retrieved. If multiple small chunks relate to a larger segment, they are merged to form a comprehensive context, which is then presented to the LLM.`
	3. **Fusion Retrieval**
		* `The concept of fusion retrieval combines traditional keyword-based search methods, like TF-IDF or BM25, with modern vector-based search techniques. This hybrid approach, often implemented using algorithms like Reciprocal Rank Fusion (RRF), optimizes retrieval by integrating diverse similarity measures.`
- **Re-Ranking & Filtering**
	* `After the initial retrieval of results using any of the aforementioned sophisticated algorithms, the focus shifts to refining these results through various post-processing techniques.`
	* `Various systems enabling the fine-tuning of retrieval outcomes based on similarity scores, keywords, metadata, or through re-ranking with additional models. These models could include an LLM, a sentence-transformer cross-encoder, or even external reranking services like Cohere. Moreover, filtering can also be adjusted based on metadata attributes, such as the recency of the data, ensuring that the most relevant and timely information is prioritized. This stage is critical as it prepares the retrieved data for the final step — feeding it into an LLM to generate the precise answer.`
	1. f
	2. f
- **Query Transformations**
	1. **(Sub-)Query Decomposition**
		* `For complex queries that are unlikely to yield direct comparisons or results from existing data (e.g., comparing GitHub stars between Langchain and LlamaIndex), an LLM can break down the query into simpler, more manageable sub-queries. Each sub-query can then be processed independently, with their results synthesized later to form a comprehensive response.`
		* Multi Query Retriever and Sub Question Query Engine
		- Step-back Prompting
			* `method involves using an LLM to generate a broader or more general query from the original, complex query. The aim is to retrieve a higher-level context that can serve as a foundation for answering the more specific original query. The contexts from both the original and the generalized queries are then combined to enhance the final answer generation.`
		- Query Rewriting
			* https://archive.is/FCiaW
			* `Another technique involves using an LLM to reformulate the initial query to improve the retrieval process`
	2. **Reference Citations**
		- Direct Source Mention
			* Require mention of source IDs directly in generated response.
		- Fuzzy Matching
			* Align portions of the response with their corresponding text chunks in the index.
		- Research:
			- Attribution Bench: https://osu-nlp-group.github.io/AttributionBench/
				* Finetuning T5 models outperform otherwise SOTA models.
				* Complexity of questions and data are issues. 
			- ContextCite: https://gradientscience.org/contextcite/
				* Hot shit?
				* https://gradientscience.org/contextcite-applications/
			- Metrics - Enabling LLMs to generate text with citations paper
				* https://arxiv.org/abs/2305.14627
- **Chat Engine**
	1. ContextChatEngine:
		* `A straightforward approach where the LLM retrieves context relevant to the user’s query along with any previous chat history. This history is then used to inform the LLM’s response, ensuring continuity and relevance in the dialogue.`
	2. CondensePlusContextMode
		* ` A more advanced technique where each interaction’s chat history and the last message are condensed into a new query. This refined query is used to retrieve relevant context, which, along with the original user message, is passed to the LLM for generating a response.`
- **Query Routing**
	* `Query routing involves strategic decision-making powered by an LLM to determine the most effective subsequent action based on the user’s query. This could include decisions to summarize information, search specific data indices, or explore multiple routes to synthesize a comprehensive answer. Query routers are crucial for selecting the appropriate data source or index, especially in systems where data is stored across multiple platforms, such as vector stores, graph databases, or relational databases.`
	- Query Routers
		* F
- **Agents in RAG Systems**
	1. **Multi-Document Agent Scheme**
	2. **Walking RAG** - Multi-shot retrieval
		- Have the LLM ask for more information as needed and perform searches for said information, to loop back in to asking the LLM if there's enough info.
		- Things necessary to facillitate:
			* We need to extract partial information from retrieved pieces of source data, so we can learn as we go.
			* We need to find new places to look, informed by the source data as well as the question.
			* We need to retrieve information from those specific places.
	3. F
- **Response Synthesizer**
	* `The simplest method might involve merely concatenating all relevant context with the query and processing it through an LLM. However, more nuanced approaches involve multiple LLM interactions to refine the context and enhance the quality of the final answer.`
	1. Iterative Refinement
		* `Breaking down the retrieved context into manageable chunks and sequentially refining the response through multiple LLM interactions.`
	2. Context Summarization
		* `Compressing the extensive retrieved context to fit within an LLM’s prompt limitations.`
	3. Multi-Answer Generation
		* `Producing several responses from different context segments and then synthesizing these into a unified answer.`
- **Evaluating RAG Performance**



- Semantic + Relevance Ranking
	- One example:
		* `rank = (cosine similarity) + (weight) x (relevance score)`
- Embedding models need to be fine-tuned to your data for best results
	* `For your Q&A system built on support docs, you very well may find that question→question comparisons will materially improve performance opposed to question→support doc. Pragmatically, you can ask ChatGPT to generate example questions for each support doc and have a human expert curate them. In essence you’d be pre-populating your own Stack Overflow.`
	- Can create semi-synthetic training data based on your documents - Want to take this “Stack Overflow” methodology one step further?
		1. For each document, ask ChatGPT to generate a list of 100 questions it can answer
    	2. These questions won’t be perfect, so for each question you generate, compute cosine similarities with each other document
    	3. Filter those questions which would rank the correct document #1 against every other document
    	4. Identify the highest-quality questions by sorting those which have the highest difference between cosine similarity of the correct document and the second ranked document
    	5. Send to human for further curation
- **Balancing Precision vs Recall**
	- List of:
	    1. Threshold Tuning: Adjusting the threshold for deciding whether a document is relevant or not can shift the balance between precision and recall. Lowering the threshold may increase recall but decrease precision, and vice versa.
    	2. Query Expansion and Refinement: Enhancing the query with additional keywords (query expansion) can increase recall by retrieving a broader set of documents. Conversely, refining the query by adding more specific terms can improve precision.
    	3. Relevance Feedback: Incorporating user feedback into the retrieval process can help refine the search results. Users' interactions with the results (clicks, time spent on a document, etc.) can provide valuable signals to adjust the balance between precision and recall.
    	4. Use of Advanced Models: Employing more sophisticated models like deep neural networks can improve both precision and recall. These models are better at understanding complex queries and documents, leading to more accurate retrieval.
    	5. Customizing Based on Use Case: Different applications may require a different balance of precision and recall. For instance, in a legal document search, precision might be more important to ensure that all retrieved documents are highly relevant. In a medical research scenario, recall might be prioritized to ensure no relevant studies are missed.






### LLM Triangle Principles


