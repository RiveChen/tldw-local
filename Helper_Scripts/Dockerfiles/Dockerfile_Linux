# Usage
# docker build -t tldw-app --build-arg GPU_SUPPORT=cpu .
# docker run -p 7860:7860 -v tldw_volume:/tldw tldw-app

# Use Nvidia image:
FROM nvidia/cuda:12.0.0-runtime-ubuntu22.04

# Use an official Python runtime as a parent image
#FROM python:3.10.15-slim-bookworm


# Set build arguments
ARG REPO_URL=https://github.com/rmusser01/tldw.git
ARG BRANCH=main
ARG GPU_SUPPORT=cpu

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsqlite3-dev \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set the working directory in the container
WORKDIR /tldw

# Clone the repository
RUN git clone -b ${BRANCH} ${REPO_URL} .

# Create and activate virtual environment
RUN python3 -m venv ./venv
ENV PATH="/tldw/venv/bin:$PATH"

# Upgrade pip and install wheel
RUN pip install --upgrade pip wheel

# Install PyTorch based on GPU support
RUN if [ "$GPU_SUPPORT" = "cuda" ]; then \
        pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu123; \
    elif [ "$GPU_SUPPORT" = "amd" ]; then \
        pip install torch-directml; \
    else \
        pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu; \
    fi

# Install other requirements
RUN pip install -r requirements.txt

# Update config.txt for CPU if needed
RUN if [ "$GPU_SUPPORT" = "cpu" ]; then \
        sed -i 's/cuda/cpu/' ./Config_Files/config.txt; \
    fi

# Create a volume for persistent storage
VOLUME /tldw

# Make port 7860 available to the world outside this container
EXPOSE 7860

# Run the application
CMD ["python3", "summarize.py", "-gui"]